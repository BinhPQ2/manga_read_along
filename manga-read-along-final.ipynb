{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nuke button","metadata":{}},{"cell_type":"code","source":"# Delete all files and folders in the working directory. Use with caution!\nimport shutil\nshutil.rmtree(\"/kaggle/working\", ignore_errors=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mamba create -n py311 -y\n# !source /opt/conda/bin/activate py311 && mamba install python=3.11 jupyter mamba -y\n\n# !sudo rm /opt/conda/bin/python3\n# !sudo ln -sf /opt/conda/envs/py311/bin/python3 /opt/conda/bin/python3\n# !sudo rm /opt/conda/bin/python3.7\n# !sudo ln -sf /opt/conda/envs/py311/bin/python3 /opt/conda/bin/python3.7\n# !sudo rm /opt/conda/bin/python\n# !sudo ln -sf /opt/conda/envs/py311/bin/python3 /opt/conda/bin/python\n# !python --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clone repo","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/\n# !git clone --single-branch --branch magiv2 https://github.com/BinhPQ2/manga_read_along.git\n!git clone https://github.com/BinhPQ2/manga_read_along.git\n%cd /kaggle/working/manga_read_along\n!git submodule init\n!git submodule update --remote","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install Library","metadata":{}},{"cell_type":"code","source":"!cat /kaggle/working/manga_read_along/requirements_kaggle.txt | xargs -n 1 pip install -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!npm install -g npm@10.9.0\n!npm install -g localtunnel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run UI","metadata":{}},{"cell_type":"code","source":"! wget -q -O - ipv4.icanhazip.com # paste the port below into the box when run the command below","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/manga_read_along\n\n# Import necessary libraries\nfrom pyngrok import ngrok\nimport subprocess\nimport time\n\n# Set ngrok authentication token\nngrok.set_auth_token(\"2mwuWCzayuo3QMhfoU9uiILskaw_DpPYrfQx1mNnZ5qYHv9p\")\n\n# Start ngrok\nngrok_process = subprocess.Popen(['ngrok', 'http', '8000'])  # Adjust the port if needed\ntime.sleep(2)  # Give ngrok some time to initialize\n\n# # # Get the public URL for ngrok\n#ngrok_url = ngrok.connect(8000)\n#print(f\"Ngrok tunnel \\\"{ngrok_url}\\\" -> \\\"http://127.0.0.1:8000\\\"\")\n\n# Start your FastAPI or Streamlit app\napp_process = subprocess.Popen(['python', 'main.py'])  # Ensure 'main.py' is the correct entry point\n\n# Start localtunnel\nlocaltunnel_process = subprocess.Popen(['npx', 'localtunnel', '--port', '8501'])\n\n# Keep the notebook running\ntry:\n    while True:\n        time.sleep(1)  # Keep the loop running\nexcept KeyboardInterrupt:\n    # Clean up on exit\n    ngrok_process.terminate()\n    app_process.terminate()\n    localtunnel_process.terminate()","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nVisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"name":"stdout","text":"Running on cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Welcome to the CBC MILP Solver \nVersion: 2.10.3 \nBuild Date: Dec 15 2019 \n\ncommand line - /opt/conda/lib/python3.10/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/f4f1cbe75ea84b6b91d9dc05830fe106-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /tmp/f4f1cbe75ea84b6b91d9dc05830fe106-pulp.sol (default strategy 1)\nAt line 2 NAME          MODEL\nAt line 3 ROWS\nAt line 146 COLUMNS\nAt line 573 RHS\nAt line 715 BOUNDS\nAt line 756 ENDATA\nProblem MODEL has 141 rows, 40 columns and 306 elements\nCoin0008I MODEL read with 0 errors\nOption for timeMode changed from cpu to elapsed\nContinuous objective value is 3.87001 - 0.00 seconds\nCgl0005I 2 SOS with 10 members\nCgl0004I processed model has 6 rows, 10 columns (10 integer (10 of which binary)) and 18 elements\nCbc0038I Initial state - 0 integers unsatisfied sum - 0\nCbc0038I Solution found of 3.87001\nCbc0038I Before mini branch and bound, 10 integers at bound fixed and 0 continuous\nCbc0038I Mini branch and bound did not improve solution (0.00 seconds)\nCbc0038I After 0.00 seconds - Feasibility pump exiting with objective of 3.87001 - took 0.00 seconds\nCbc0012I Integer solution of 3.8700051 found by feasibility pump after 0 iterations and 0 nodes (0.00 seconds)\nCbc0001I Search completed - best objective 3.8700051057679, took 0 iterations and 0 nodes (0.00 seconds)\nCbc0035I Maximum depth 0, 0 variables fixed on reduced cost\nCuts at root node changed objective from 3.87001 to 3.87001\nProbing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nGomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nKnapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nClique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nMixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nFlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nTwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n\nResult - Optimal solution found\n\nObjective value:                3.87000511\nEnumerated nodes:               0\nTotal iterations:               0\nTime (CPU seconds):             0.00\nTime (Wallclock seconds):       0.00\n\nOption for printingOptions changed from normal to all\nTotal time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n\nDone you WEEEEB!\nSetting up text-to-speech...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.speakers = torch.load(speaker_file_path)\n/opt/conda/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":" > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n > Using model: xtts\nRunning text_to_speech.py...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.speakers = torch.load(speaker_file_path)\n/opt/conda/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\nGPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Running on cuda\n > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n > Using model: xtts\nProcessing character 'mom' with text: But why did they...\n > Text splitted to sentences.\n['But why did they...']\n > Processing time: 2.8216392993927\n > Real-time factor: 0.4023354019115949\nProcessing character 'mom' with text: Ah, that's because you're not a human.\n > Text splitted to sentences.\n[\"Ah, that's because you're not a human.\"]\n > Processing time: 1.8937795162200928\n > Real-time factor: 0.35378404443416234\nProcessing character 'mom' with text: Since your Dad\n > Text splitted to sentences.\n['Since your Dad']\n > Processing time: 0.622861385345459\n > Real-time factor: 0.3250211460352937\nProcessing character 'mom' with text: Is a Dragon\n > Text splitted to sentences.\n['Is a Dragon']\n > Processing time: 0.832202672958374\n > Real-time factor: 0.33022726998870117\nProcessing character 'mom' with text: According to your Dad, your body will start to change when you're 15.\n > Text splitted to sentences.\n[\"According to your Dad, your body will start to change when you're 15.\"]\n > Processing time: 2.2444591522216797\n > Real-time factor: 0.3501806033233898\nProcessing character 'ruri' with text: I didn't expect the horns to grow overnight though.\n > Text splitted to sentences.\n[\"I didn't expect the horns to grow overnight though.\"]\n > Processing time: 1.3814096450805664\n > Real-time factor: 0.3509226114519181\nProcessing character 'ruri' with text: Anyways, eat your breakfast.\n > Text splitted to sentences.\n['Anyways, eat your breakfast.']\n > Processing time: 1.1305263042449951\n > Real-time factor: 0.3345515488592729\nAudio files have been saved to /kaggle/working/output/audio\nRunning main.py...\n","output_type":"stream"},{"name":"stderr","text":"ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from '/kaggle/working/output/output_final/video_Padding_True.mp4':\n  Metadata:\n    major_brand     : isom\n    minor_version   : 512\n    compatible_brands: isomiso2mp41\n    encoder         : Lavf59.27.100\n  Duration: 00:00:31.92, start: 0.000000, bitrate: 1205 kb/s\n  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 822x524 [SAR 1:1 DAR 411:262], 1204 kb/s, 24 fps, 24 tbr, 12288 tbn, 24 tbc (default)\n    Metadata:\n      handler_name    : VideoHandler\n      vendor_id       : [0][0][0][0]\nGuessed Channel Layout for Input Stream #1.0 : mono\nInput #1, wav, from '/kaggle/working/output/output_final/temp_audio.wav':\n  Duration: 00:00:32.05, bitrate: 384 kb/s\n  Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, mono, s16, 384 kb/s\nStream mapping:\n  Stream #0:0 -> #0:0 (copy)\n  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\nPress [q] to stop, [?] for help\nOutput #0, mp4, to '/kaggle/working/output/output_final/video_Padding_True_audio.mp4':\n  Metadata:\n    major_brand     : isom\n    minor_version   : 512\n    compatible_brands: isomiso2mp41\n    encoder         : Lavf58.76.100\n  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 822x524 [SAR 1:1 DAR 411:262], q=2-31, 1204 kb/s, 24 fps, 24 tbr, 12288 tbn, 12288 tbc (default)\n    Metadata:\n      handler_name    : VideoHandler\n      vendor_id       : [0][0][0][0]\n  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 24000 Hz, mono, fltp, 69 kb/s\n    Metadata:\n      encoder         : Lavc58.134.100 aac\nframe=  766 fps=0.0 q=-1.0 Lsize=    4937kB time=00:00:31.91 bitrate=1267.3kbits/s speed= 169x    \nvideo:4694kB audio:229kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.281065%\n[aac @ 0x589b064a03c0] Qavg: 16820.127\n","output_type":"stream"},{"name":"stdout","text":"Deleted images\nProcessed and saved modified panel view images for: 000.jpg\nVideo saved as: /kaggle/working/output/output_final/video_Padding_True.mp4\nAudio file not found for image: page_000_panel_000_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_000_panel_001_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_000_panel_002_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_000_panel_003_bubble_000.jpg. Appending silence for 1 seconds.\nAudio merged successfully into video saved as: /kaggle/working/output/output_final/video_Padding_True_audio.mp4\nTemporary audio file '/kaggle/working/output/output_final/temp_audio.wav' deleted. Original video '/kaggle/working/output/output_final/video_Padding_True.mp4' deleted.\nRe-encoding final video...\nRe-encoding completed successfully.\n\nPipeline completed successfully!\nResponse is_success: True\nINFO:     127.0.0.1:33094 - \"POST /generate-manga HTTP/1.1\" 200 OK\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"raw_image_path = \"input/raw\"\ncharacter_path = \"input/character\"\n\nraw_image_rename_path = \"/kaggle/working/output/renamed\"\ncolorized_path = \"/kaggle/working/output/colorized\"\njson_path = \"/kaggle/working/output/json\"\ntranscript_path = \"/kaggle/working/output/transcript\"\naudio_path = \"/kaggle/working/output/audio\"\nfinal_output_path =\"/kaggle/working/output/output_final\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voice_bank = \"/kaggle/working/manga_read_along/input/voice_bank\"\nmale_character = ['teacher']\ntranscript_file = f\"{transcript_path}/transcript.txt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/manga_read_along/magi_functional/text_to_speech.py -i {raw_image_rename_path} -v {voice_bank} -t {transcript_file} -o {audio_path}  -m male_character ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Edit Kaggle","metadata":{}},{"cell_type":"code","source":"process_file = '''    \nimport os\nimport shutil\nimport subprocess\nimport torch\nfrom transformers import AutoModel\nfrom TTS.api import TTS\nfrom unittest.mock import patch\n\ndef pipeline(is_colorization: bool, is_panel_view: bool):\n    root_path = \"/kaggle/working/\"\n\n    raw_image_path = os.path.join(root_path, \"manga_read_along/input/raw\")\n    character_path = os.path.join(root_path, \"manga_read_along/input/character\")\n    voice_bank = os.path.join(root_path, \"manga_read_along/input/voice_bank\")\n\n    output_path = os.path.join(root_path, \"output\")\n    raw_image_rename_path = os.path.join(output_path, \"renamed\")\n    colorized_path = os.path.join(output_path, \"colorized\")\n    json_path = os.path.join(output_path, \"json\")\n    transcript_path = os.path.join(output_path, \"transcript\")\n    transcript_file = os.path.join(transcript_path, \"transcript.txt\")\n    audio_path = os.path.join(output_path, \"audio\")\n    final_output_path = os.path.join(output_path, \"output_final\")\n\n    if os.path.exists(output_path):\n        shutil.rmtree(output_path)\n\n    os.makedirs(output_path, exist_ok=True)\n    os.makedirs(raw_image_rename_path, exist_ok=True)\n    os.makedirs(colorized_path, exist_ok=True)\n    os.makedirs(json_path, exist_ok=True)\n    os.makedirs(transcript_path, exist_ok=True)\n    os.makedirs(audio_path, exist_ok=True)\n    os.makedirs(final_output_path, exist_ok=True)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Running on {device}\", flush=True)\n\n    print(\"Initializing submodules...\", flush=True)\n    for submodule in [\"manga_read_along\", \"manga_read_along/manga-colorization-v2-custom\"]:\n        submodule_path = os.path.join(root_path, submodule)\n        result = subprocess.run([\"git\", \"submodule\", \"init\"], cwd=submodule_path)\n        if result.returncode != 0:\n            print(f\"Error initializing submodule {submodule}: {result.stderr}\")\n        result = subprocess.run([\"git\", \"submodule\", \"update\", \"--remote\"], cwd=submodule_path)\n        if result.returncode != 0:\n            print(f\"Error updating submodule {submodule}: {result.stderr}\")\n\n    print(\"Loading MAGI model...\", flush=True)\n    magiv2 = AutoModel.from_pretrained(\"ragavsachdeva/magiv2\", trust_remote_code=True).to(device).eval()\n\n    colorization_model_path = os.path.join(root_path, \"manga_read_along/manga-colorization-v2-custom/networks/generator.zip\")\n    denoising_model_path = os.path.join(root_path, \"manga_read_along/manga-colorization-v2-custom/denoising/models/net_rgb.pth\")\n    if not os.path.exists(colorization_model_path):\n        print(\"Downloading colorization model weights...\", flush=True)\n        subprocess.run([\n            \"gdown\", \"1qmxUEKADkEM4iYLp1fpPLLKnfZ6tcF-t\",\n            \"-O\", colorization_model_path\n        ])\n    if not os.path.exists(denoising_model_path):\n        os.makedirs(os.path.dirname(denoising_model_path), exist_ok=True)\n        subprocess.run([\n            \"gdown\", \"161oyQcYpdkVdw8gKz_MA8RD-Wtg9XDp3\",\n            \"-O\", denoising_model_path\n        ])\n  \n        \n    # Step 1: Run magiv2.py\n    print(\"Running magiv2.py...\", flush=True)\n    result = subprocess.run([\n        \"python\", os.path.join(root_path, \"manga_read_along/magi_functional/magiv2.py\"),\n        \"--image\", raw_image_path,\n        \"--rename_image\", raw_image_rename_path,\n        \"--character\", character_path,\n        \"--json\", json_path,\n        \"--transcript\", transcript_path\n    ])\n    if result.returncode != 0:\n        print(f\"Error in magiv2.py: {result.stderr}\", flush=True)\n    \n    if is_colorization:\n        # Step 2: Run inference_v2.py for colorization\n        print(\"Running inference_v2.py...\", flush=True)\n        result = subprocess.run([\n            \"python\", os.path.join(root_path, \"manga_read_along/manga-colorization-v2-custom/inference_v2.py\"),\n            \"-p\", raw_image_rename_path,\n            \"-des_path\", denoising_model_path,\n            \"-gen\", colorization_model_path,\n            \"-s\", colorized_path,\n            \"-ds\", \"0\",\n            \"--gpu\"\n        ])\n\n        input_path_for_combine_step = colorized_path\n    else:\n        input_path_for_combine_step = raw_image_rename_path\n        if result.returncode != 0:\n            print(f\"Error in inference_v2.py: {result.stderr}\", flush=True)\n    \n    # Step 3: Text-to-speech\n    print(\"Setting up text-to-speech...\", flush=True)\n    with patch('builtins.input', return_value='y'):\n        try:\n            tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n        except Exception as e:\n            print(f\"Error during TTS setup: {e}\", flush=True)\n\n    print(\"Running text_to_speech.py...\", flush=True)\n    result = subprocess.run([\n        \"python\", os.path.join(root_path, \"manga_read_along/magi_functional/text_to_speech.py\"),\n        \"-i\", raw_image_rename_path,\n        \"-v\", voice_bank,\n        \"-t\", transcript_file,\n        \"-o\", audio_path,\n        \"-m\", \"male_character\"\n    ])\n\n    if result.returncode != 0:\n        print(f\"Error in text_to_speech.py: {result.stderr}\", flush=True)\n\n    # Step 4: Combine in main.py\n    print(\"Running main.py...\", flush=True)\n    result = subprocess.run([\n        \"python\", os.path.join(root_path, \"manga_read_along/magi_functional/main.py\"),\n        \"-i\", input_path_for_combine_step,\n        \"-j\", json_path,\n        \"-a\", audio_path,\n        \"-s\", final_output_path, \n        \"-panel\", str(is_panel_view)\n    ])\n    if result.returncode != 0:\n        print(f\"Error in main_final.py: {result.stderr}\", flush=True)\n\n    # Step 5: Re-encode final video\n    generated_video_path = os.path.join(final_output_path, \"video_Padding_True_audio.mp4\")  # Use final output with audio merged\n    reencoded_video_path = os.path.splitext(generated_video_path)[0] + \"_reencoded.mp4\"\n    if os.path.exists(generated_video_path):\n        print(\"Re-encoding final video...\", flush=True)\n        try:\n            result = subprocess.run([\n                \"ffmpeg\", \"-i\", generated_video_path, \"-c:v\", \"libx264\", \"-c:a\", \"aac\",\n                \"-strict\", \"experimental\", reencoded_video_path\n            ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            print(\"Re-encoding completed successfully.\", flush=True)\n            print(result.stdout.decode())\n        except subprocess.CalledProcessError as e:\n            print(f\"Error during video re-encoding: {e}\", flush=True)\n            print(e.stderr.decode())\n            return False\n    else:\n        print(\"Generated video not found for re-encoding.\", flush=True)\n        return False\n    \n    print(\"Pipeline completed successfully!\", flush=True)\n    return True\n\nif __name__ == \"__main__\":\n    pipeline(is_colorization=True)\n'''\n\nwith open(\"/kaggle/working/manga_read_along/src/backend/pipeline.py\", \"w\") as file:\n    file.write(process_file)\n    print(\"Done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}