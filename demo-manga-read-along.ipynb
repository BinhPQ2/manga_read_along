{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nuke button","metadata":{}},{"cell_type":"code","source":"# Delete all files and folders in the working directory. Use with caution!\nimport shutil\nshutil.rmtree(\"/kaggle/working\", ignore_errors=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clone repo","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n# !git clone https://github.com/BinhPQ2/manga_read_along.git\n!git clone -b magiv2 https://github.com/BinhPQ2/manga_read_along.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-01T21:15:03.116793Z","iopub.execute_input":"2024-11-01T21:15:03.117374Z","iopub.status.idle":"2024-11-01T21:15:06.957085Z","shell.execute_reply.started":"2024-11-01T21:15:03.117319Z","shell.execute_reply":"2024-11-01T21:15:06.956053Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'manga_read_along'...\nremote: Enumerating objects: 620, done.\u001b[K\nremote: Counting objects: 100% (76/76), done.\u001b[K\nremote: Compressing objects: 100% (50/50), done.\u001b[K\nremote: Total 620 (delta 24), reused 65 (delta 19), pack-reused 544 (from 1)\u001b[K\nReceiving objects: 100% (620/620), 80.20 MiB | 46.69 MiB/s, done.\nResolving deltas: 100% (26/26), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/manga_read_along","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:15:08.388094Z","iopub.execute_input":"2024-11-01T21:15:08.388896Z","iopub.status.idle":"2024-11-01T21:15:08.395338Z","shell.execute_reply.started":"2024-11-01T21:15:08.388854Z","shell.execute_reply":"2024-11-01T21:15:08.394435Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/manga_read_along\n","output_type":"stream"}]},{"cell_type":"code","source":"!git submodule init\n!git submodule update --remote","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:15:10.169718Z","iopub.execute_input":"2024-11-01T21:15:10.170387Z","iopub.status.idle":"2024-11-01T21:15:24.597862Z","shell.execute_reply.started":"2024-11-01T21:15:10.170347Z","shell.execute_reply":"2024-11-01T21:15:24.596700Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Submodule 'magi_functional' (https://github.com/BinhPQ2/magi_functional.git) registered for path 'magi_functional'\nSubmodule 'manga-colorization-v2-custom' (https://github.com/BinhPQ2/manga-colorization-v2-custom.git) registered for path 'manga-colorization-v2-custom'\nCloning into '/kaggle/working/manga_read_along/magi_functional'...\nCloning into '/kaggle/working/manga_read_along/manga-colorization-v2-custom'...\nSubmodule path 'magi_functional': checked out '56538918e082d03a2a809a623ce470659492334a'\nSubmodule path 'manga-colorization-v2-custom': checked out 'cace2f95916c684f614554feac04caa6c4780a12'\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -qr /kaggle/working/manga_read_along/magi_functional/requirements_kaggle.txt","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:24:18.132630Z","iopub.execute_input":"2024-10-31T16:24:18.132941Z","iopub.status.idle":"2024-10-31T16:24:30.257116Z","shell.execute_reply.started":"2024-10-31T16:24:18.132909Z","shell.execute_reply":"2024-10-31T16:24:30.255966Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Check GPU and set device","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\nif torch.cuda.is_available():\n    !nvidia-smi\n    !nvcc --version\nelse:\n    print(\"GPU is not available\")\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:15:24.600231Z","iopub.execute_input":"2024-11-01T21:15:24.600690Z","iopub.status.idle":"2024-11-01T21:15:29.914308Z","shell.execute_reply.started":"2024-11-01T21:15:24.600642Z","shell.execute_reply":"2024-11-01T21:15:29.912930Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Fri Nov  1 21:15:28 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0             28W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Wed_Nov_22_10:17:15_PST_2023\nCuda compilation tools, release 12.3, V12.3.107\nBuild cuda_12.3.r12.3/compiler.33567101_0\nRunning on cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Download MAGI model","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nfrom transformers import AutoModel\nimport torch\n\nmodel = AutoModel.from_pretrained(\"ragavsachdeva/magiv2\", trust_remote_code=True).to(device).eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download weight for colorization","metadata":{}},{"cell_type":"code","source":"!gdown 1qmxUEKADkEM4iYLp1fpPLLKnfZ6tcF-t -O /kaggle/working/manga_read_along/manga-colorization-v2-custom/networks/generator.zip\n!mkdir /kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models\n!gdown 161oyQcYpdkVdw8gKz_MA8RD-Wtg9XDp3 -O /kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models/net_rgb.pth","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:24:30.259064Z","iopub.execute_input":"2024-10-31T16:24:30.259391Z","iopub.status.idle":"2024-10-31T16:24:41.794486Z","shell.execute_reply.started":"2024-10-31T16:24:30.259359Z","shell.execute_reply":"2024-10-31T16:24:41.793378Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1qmxUEKADkEM4iYLp1fpPLLKnfZ6tcF-t\nFrom (redirected): https://drive.google.com/uc?id=1qmxUEKADkEM4iYLp1fpPLLKnfZ6tcF-t&confirm=t&uuid=5fa29fdd-a87d-4ca5-a991-0584a82351e8\nTo: /kaggle/working/manga_read_along/manga-colorization-v2-custom/networks/generator.zip\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129M/129M [00:01<00:00, 81.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=161oyQcYpdkVdw8gKz_MA8RD-Wtg9XDp3\nTo: /kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models/net_rgb.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.44M/3.44M [00:00<00:00, 189MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Magiv2","metadata":{}},{"cell_type":"markdown","source":"## Generate json and transcipt file","metadata":{}},{"cell_type":"markdown","source":"#### Create raw and character/names list (should apply rename first dumbass, TODO)","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\n\ndef create_chapter_pages_and_character_bank(manga_folder, character_folder):\n    # Create lists for chapter pages and character bank\n    chapter_pages = []\n    character_bank = {\n        \"images\": [],\n        \"names\": []\n    }\n\n#     Iterate through manga images to create chapter_pages\n    for image_file in os.listdir(manga_folder):\n        if image_file.endswith(('.png', '.jpg', '.jpeg')):  # Check for image file extensions\n            # Extract the page number using regex\n            match = re.search(r'p(\\d+)', image_file)\n            if match:\n                page_number = int(match.group(1))  # Convert to integer for sorting\n                chapter_pages.append((page_number, image_file))  # Store as tuple (page_number, image_file)\n            else:\n                page_number = image_file.rsplit(\".\", 1)[0]\n                chapter_pages.append((page_number, image_file))  # Store as tuple (page_number, image_file)\n\n    # Sort chapter pages by page number\n    chapter_pages.sort(key=lambda x: x[0])\n    chapter_pages = [os.path.join(manga_folder, img[1]) for img in chapter_pages]  # Extract just the filenames after sorting\n\n    # Iterate through character images to create character bank\n    for char_image_file in os.listdir(character_folder):\n        if char_image_file.endswith(('.png', '.jpg', '.jpeg')):  # Check for image file extensions\n            # Split the filename to extract character name\n            char_name = char_image_file.split('_')[0]  # Get the part before the underscore\n            character_bank[\"images\"].append(os.path.join(character_folder, char_image_file))\n            character_bank[\"names\"].append(char_name)\n    return chapter_pages, character_bank\n\n# Define your folders\n# One Piece\n# manga_folder = Path(\"/kaggle/working/magi_functional/data_test/personal_data/One_Piece/raw_manga\")\n# character_folder = Path(\"/kaggle/working/magi_functional/data_test/personal_data/One_Piece/character\")\n\n# Ruri Dragon\nmanga_folder = Path(\"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/raw\")\ncharacter_folder = Path(\"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/character\")\n\n# Get chapter pages and character bank\nchapter_pages_original, character_bank_original = create_chapter_pages_and_character_bank(manga_folder, character_folder)\n\nchapter_pages_test = chapter_pages_original[:]\ncharacter_bank_test = character_bank_original\n\n# Print the results (for debugging)\nprint(\"Chapter Pages:\")\nprint(chapter_pages_test)\n\nprint(\"\\nCharacter Bank:\")\nprint(character_bank_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Process (OCR â†’ Transcript)","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\ndef read_image(path_to_image):\n    with open(path_to_image, \"rb\") as file:\n        image = Image.open(file).convert(\"L\").convert(\"RGB\")\n        image = np.array(image)\n    return image\n\n\nchapter_pages = [read_image(x) for x in chapter_pages_test]\ncharacter_bank = character_bank_test.copy()\ncharacter_bank[\"images\"] = [read_image(x) for x in character_bank_test[\"images\"]]\n\nwith torch.no_grad():\n    per_page_results = model.do_chapter_wide_prediction(chapter_pages, character_bank, use_tqdm=True, do_ocr=True)\n\nprint(\"Continue with next cell\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save transcript and json file","metadata":{}},{"cell_type":"code","source":"result_dir = \"/kaggle/working/result\"\njson_output_dir = f\"{result_dir}/json_results\"\nresult_image_output_dir = f\"{result_dir}/image_results\"\ntranscript_output_dir = f\"{result_dir}/transcript.txt\"\nos.makedirs(json_output_dir, exist_ok=True)  # Create the directory if it doesn't exist  \nos.makedirs(result_image_output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\ntranscript = []\nfor i, (image, page_result) in enumerate(zip(chapter_pages, per_page_results)):\n    image_name_ext = os.path.basename(chapter_pages_test[i]) \n    # Split the image name and its extension\n    image_name, image_extension = os.path.splitext(image_name_ext)\n    \n#     model.visualise_single_image_prediction(image, page_result, os.path.join(result_image_output_dir, f\"{image_name}.png\")) # enable this if you want to see the result image included with all the annotation box\n    # Save page_result to JSON\n    json_file_path = os.path.join(json_output_dir, f\"{image_name}.json\")\n    with open(json_file_path, 'w') as json_file:\n        json.dump(page_result, json_file, indent=4)\n\n    speaker_name = {\n        text_idx: page_result[\"character_names\"][char_idx] for text_idx, char_idx in page_result[\"text_character_associations\"]\n    }\n    \n    transcript.append(f\"<page>{image_name}<endpage>\")\n    for j in range(len(page_result[\"ocr\"])):\n        if not page_result[\"is_essential_text\"][j]:\n            continue\n        name = speaker_name.get(j, \"unsure\") \n        transcript.append(f\"<name>{name}<endname>: {page_result['ocr'][j]}\")\nwith open(transcript_output_dir, \"w\") as fh:\n    for line in transcript:\n        fh.write(line + \"\\n\")\n\nprint(\"\\n\\nDone you WEEEEB!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate video from Magi json file","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/manga_read_along/magi_functional/main.py","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:41:45.560386Z","iopub.execute_input":"2024-10-31T16:41:45.560750Z","iopub.status.idle":"2024-10-31T16:42:40.008663Z","shell.execute_reply.started":"2024-10-31T16:41:45.560713Z","shell.execute_reply":"2024-10-31T16:42:40.007657Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Deleted images\nProcessed and saved modified full page images for: 0000.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0000.json\nProcessed and saved modified full page images for: 0001.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0001.json\nProcessed and saved modified full page images for: 0002.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0002.json\nProcessed and saved modified full page images for: 0003.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0003.json\nProcessed and saved modified full page images for: 0004.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0004.json\nProcessed and saved modified full page images for: 0005.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0005.json\nProcessed and saved modified full page images for: 0006.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0006.json\nProcessed and saved modified full page images for: 0007.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0007.json\nProcessed and saved modified full page images for: 0008.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0008.json\nProcessed and saved modified full page images for: 0009.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0009.json\nProcessed and saved modified full page images for: 0010.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0010.json\nProcessed and saved modified full page images for: 0011.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0011.json\nProcessed and saved modified full page images for: 0012.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0012.json\nProcessed and saved modified full page images for: 0013.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0013.json\nProcessed and saved modified full page images for: 0014.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0014.json\nProcessed and saved modified full page images for: 0015.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0015.json\nProcessed and saved modified full page images for: 0016.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0016.json\nProcessed and saved modified full page images for: 0017.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0017.json\nProcessed and saved modified full page images for: 0018.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0018.json\nProcessed and saved modified full page images for: 0019.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0019.json\nProcessed and saved modified full page images for: 0020.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0020.json\nProcessed and saved modified full page images for: 0021.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0021.json\nProcessed and saved modified full page images for: 0022.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0022.json\nProcessed and saved modified full page images for: 0023.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0023.json\nProcessed and saved modified full page images for: 0024.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0024.json\nProcessed and saved modified full page images for: 0025.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0025.json\nProcessed and saved modified full page images for: 0026.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0026.json\nProcessed and saved modified full page images for: 0027.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0027.json\nProcessed and saved modified full page images for: 0028.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0028.json\nProcessed and saved modified full page images for: 0029.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0029.json\nProcessed and saved modified full page images for: 0030.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0030.json\nProcessed and saved modified full page images for: 0031.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0031.json\nProcessed and saved modified full page images for: 0032.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0032.json\nProcessed and saved modified full page images for: 0033.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0033.json\nProcessed and saved modified full page images for: 0034.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0034.json\nProcessed and saved modified full page images for: 0035.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0035.json\nProcessed and saved modified full page images for: 0036.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0036.json\nProcessed and saved modified full page images for: 0037.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0037.json\nProcessed and saved modified full page images for: 0038.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0038.json\nProcessed and saved modified full page images for: 0039.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0039.json\nProcessed and saved modified full page images for: 0040.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0040.json\nProcessed and saved modified full page images for: 0041.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0041.json\nProcessed and saved modified full page images for: 0042.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0042.json\nProcessed and saved modified full page images for: 0043.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0043.json\nProcessed and saved modified full page images for: 0044.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0044.json\nProcessed and saved modified full page images for: 0045.jpg using json file: magi_functional/data_test/personal_data/Ruri_Dragon/json_results/renamed/0045.json\nVideo saved as: test_output_final/video_Padding_True.mp4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Download image result","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"image_results\", \"/kaggle/working/manga_read_along/test_output_final\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:44:21.903680Z","iopub.execute_input":"2024-10-31T16:44:21.904386Z","iopub.status.idle":"2024-10-31T16:44:28.557512Z","shell.execute_reply.started":"2024-10-31T16:44:21.904347Z","shell.execute_reply":"2024-10-31T16:44:28.556658Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/image_results.zip","text/html":"<a href='image_results.zip' target='_blank'>image_results.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Colorization","metadata":{}},{"cell_type":"markdown","source":"In THEORY, this should work (haven't tested yet)","metadata":{}},{"cell_type":"code","source":"!python \"/kaggle/working/manga_read_along/manga-colorization-v2-custom/main.py\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"or run the below code","metadata":{}},{"cell_type":"code","source":"!python \"/kaggle/working/manga_read_along/manga-colorization-v2-custom/inference_v2.py\" -p \"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/raw/\" -des_path \"/kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models/net_rgb.pth\" -gen \"/kaggle/working/manga_read_along/manga-colorization-v2-custom/networks/generator.zip\" -s \"./test_output_coloring\" -ds 0 --gpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text to Speech","metadata":{}},{"cell_type":"markdown","source":"## Install Library","metadata":{}},{"cell_type":"code","source":"!pip install -q TTS\n!pip install -q pydub","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:19:26.671757Z","iopub.execute_input":"2024-11-01T21:19:26.672751Z","iopub.status.idle":"2024-11-01T21:21:09.480958Z","shell.execute_reply.started":"2024-11-01T21:19:26.672706Z","shell.execute_reply":"2024-11-01T21:21:09.479782Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\nalbucore 0.0.17 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\nalbumentations 1.4.17 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\narviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nbayesian-optimization 1.5.1 requires numpy>=1.25, but you have numpy 1.22.0 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.22.0 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ncudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\ncudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-cuda 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\ndask-cudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\ndask-cudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-expr 1.1.15 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\ndipy 1.9.0 requires numpy>=1.22.4, but you have numpy 1.22.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmizani 0.11.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nmizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nmne 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.0 which is incompatible.\nnumexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.22.0 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npylibraft 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\npywavelets 1.6.0 requires numpy<3,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\nraft-dask 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\nrmm 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nscikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nstatsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.22.0 which is incompatible.\ntensorflow 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.0 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nucx-py 0.39.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nucxx 0.39.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nxarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\nxarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\nxarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import librosa\nimport librosa\nimport numpy as np\nimport soundfile as sf\nfrom IPython.display import Audio\nfrom TTS.api import TTS\nfrom unittest.mock import patch\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:21:27.168557Z","iopub.execute_input":"2024-11-01T21:21:27.169344Z","iopub.status.idle":"2024-11-01T21:21:27.175346Z","shell.execute_reply.started":"2024-11-01T21:21:27.169280Z","shell.execute_reply":"2024-11-01T21:21:27.174491Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Mock input to automatically respond with 'y'\nwith patch('builtins.input', return_value='y'):\n    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:21:27.177073Z","iopub.execute_input":"2024-11-01T21:21:27.177370Z","iopub.status.idle":"2024-11-01T21:22:38.567543Z","shell.execute_reply.started":"2024-11-01T21:21:27.177338Z","shell.execute_reply":"2024-11-01T21:22:38.566719Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":" > You must confirm the following:\n | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.86G/1.87G [00:43<00:00, 42.9MiB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:44<00:00, 42.2MiB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.37k/4.37k [00:00<00:00, 23.3kiB/s]\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 361k/361k [00:00<00:00, 1.59MiB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.0/32.0 [00:00<00:00, 130iB/s]\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3.90M/7.75M [00:00<00:00, 39.0MiB/s]","output_type":"stream"},{"name":"stdout","text":" > Model's license - CPML\n > Check https://coqui.ai/cpml.txt for more info.\n > Using model: xtts\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.speakers = torch.load(speaker_file_path)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.75M/7.75M [00:15<00:00, 39.0MiB/s]/opt/conda/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\nGPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Remember to add RAVDESS Emotional speech audio (Female datasets to kaggle input, yes it's stupid af but its a bandage solution for now ãƒã‚«)","metadata":{}},{"cell_type":"markdown","source":"## Code","metadata":{}},{"cell_type":"code","source":"voice_mapping = {\n'other': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-01-01-01-01-02.wav\",\n'ruri': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_04/03-01-01-01-01-01-04.wav\",\n'teacher': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\",\n'mom': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_08/03-01-01-01-01-01-08.wav\",\n'unsure': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_10/03-01-01-01-01-01-10.wav\",\n'ukka': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_12/03-01-01-01-01-01-12.wav\"\n}\n ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from magi_functional.utils.utils import rename_image_to_correct_format, get_digit_number_for_name_format, generate_name_format\n\nimages_folder = '/kaggle/working/manga_read_along/test_output_coloring'\n\nnumber_of_digit_for_name = get_digit_number_for_name_format(images_folder)\nname_format = generate_name_format(number_of_digit_for_name)\nprint(name_format)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:23:16.545507Z","iopub.execute_input":"2024-11-01T21:23:16.546415Z","iopub.status.idle":"2024-11-01T21:23:16.552771Z","shell.execute_reply.started":"2024-11-01T21:23:16.546365Z","shell.execute_reply":"2024-11-01T21:23:16.551792Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"page_{:04}_panel_{:04}_bubble_{:04}{}\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport re\nfrom pydub import AudioSegment\nimport shutil\n\n# Function to parse the transcript from a text file\ndef parse_transcript(transcript_path, line_end: int = None):\n    pages = []\n    current_page = None\n\n    with open(transcript_path, 'r') as file:\n        content = file.readlines()\n        \n    lines_to_process = content[:line_end] if line_end is not None else content\n    \n    for line in lines_to_process:\n        if line.startswith(\"<page>\"):\n            # Start a new page\n            if current_page is not None:\n                pages.append(current_page)\n            current_page = {\"page\": line[6:8], \"lines\": []}  # Extract page number \n        elif line.startswith(\"<name>\") and current_page is not None:\n            # Extract character name and dialogue\n            match = re.match(r\"<name>([^<]+)<endname>:\\s*(.+)\", line)\n            if match:\n                character = match.group(1).lower()  # Lowercase for consistency\n                dialogue = match.group(2)\n                current_page[\"lines\"].append((character, dialogue))\n\n    # Add the last page if it exists\n    if current_page is not None:\n        pages.append(current_page)\n        \n    return pages\n\n# Function to get all voice files from the specified directory\ndef get_voice_files(directory):\n    all_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".wav\"):\n                all_files.append(os.path.join(root, file))\n    return all_files\n\n# Function to filter voice files into male and female categories\ndef filter_voice_files(files):\n    male_files = []\n    female_files = []\n    \n    for file in files:\n        # Extract the actor number from the filename\n        match = re.search(r\"(\\d{2})\\.wav$\", file)\n        if match:\n            actor_number = int(match.group(1))\n            if actor_number % 2 == 0:  # Even numbers are female\n                female_files.append(file)\n            else:  # Odd numbers are male\n                male_files.append(file)\n                \n    return male_files, female_files\n\n# Function to randomly select voice files for characters\ndef select_voice_files_for_characters(characters):\n    all_files = get_voice_files(voice_bank)\n    male_files, female_files = filter_voice_files(all_files)\n\n    selected_files = {}\n    used_files = set()  # To keep track of used voice files\n\n    for character in characters:\n        if character in male_characters:\n            # Select a random male voice that hasn't been used\n            available_male_files = list(set(male_files) - used_files)\n            if available_male_files:\n                selected_files[character] = random.choice(available_male_files)\n                used_files.add(selected_files[character])\n        else:\n            # Select a random female voice that hasn't been used, if available\n            available_female_files = list(set(female_files) - used_files)\n            if available_female_files:\n                selected_files[character] = random.choice(available_female_files)\n                used_files.add(selected_files[character])\n            else:\n                # Select any remaining file for other characters, ensuring it's not already used\n                available_files = list(set(all_files) - used_files)\n                if available_files:\n                    selected_files[character] = random.choice(available_files)\n                    used_files.add(selected_files[character])\n    \n    return selected_files\n\n# Function to convert text to speech for a character\ndef voice_character(character, text, page_number, bubble_number, selected_files, save_directory):\n    speaker_wav = selected_files.get(character)\n\n    if speaker_wav:\n        audio_output_filename = name_format.format(int(page_number), 0, bubble_number+1, \".wav\")\n        output_filename = os.path.join(save_directory, audio_output_filename)\n        output = tts.tts_to_file(text=text, speaker_wav=speaker_wav, language=\"en\")\n        os.rename(\"output.wav\", output_filename)  # Rename the default output file\n        return output_filename\n    else:\n        raise ValueError(f\"Character '{character}' not found in voice mapping.\")\n\n# Function to process the transcript and create audio files\ndef text2speech(pages, selected_files, save_directory):\n    output_files = []\n    time_file = []\n\n    for page in pages:\n        page_number = page[\"page\"]\n        for bubble_number, (character, dialogue) in enumerate(page[\"lines\"]):\n            try:\n                output = voice_character(character, dialogue, page_number, bubble_number, selected_files, save_directory)\n                output_files.append(output)\n            except ValueError as e:\n                print(e)\n\n    return output_files\n\ndef duration_calculation():\n    audio = AudioSegment.from_file(output_filename)\n    duration_in_seconds = len(audio) / 1000\n    return duration_in_seconds","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:51:33.867194Z","iopub.execute_input":"2024-11-01T21:51:33.867644Z","iopub.status.idle":"2024-11-01T21:51:33.892583Z","shell.execute_reply.started":"2024-11-01T21:51:33.867601Z","shell.execute_reply":"2024-11-01T21:51:33.891496Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Define the directory where the RAVDESS files are located\nvoice_bank = \"/kaggle/input/ravdess-emotional-speech-audio/\"\n# Define male characters for the check\nmale_characters = ['teacher']\ntranscript_path = \"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/transcript.txt\"\n\nsave_directory = \"/kaggle/working/audio_output/\"\n\nif os.path.exists(save_directory):\n    shutil.rmtree(save_directory)\nos.makedirs(save_directory, exist_ok=True)\n\n# Get unique characters from the transcript\npages = parse_transcript(transcript_path, line_end = 20) # delete line_end to get audio of all page\ncharacters = {line[0] for page in pages for line in page[\"lines\"]}  # Get unique characters\n\n# Select voice files for characters\nselected_files = select_voice_files_for_characters(characters)\n\n# # Call the function to process the transcript and generate audio files\noutput_files = text2speech(pages, selected_files, save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:53:25.589338Z","iopub.execute_input":"2024-11-01T21:53:25.589760Z","iopub.status.idle":"2024-11-01T21:53:39.227089Z","shell.execute_reply.started":"2024-11-01T21:53:25.589721Z","shell.execute_reply":"2024-11-01T21:53:39.226049Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":" > Text splitted to sentences.\n['Eh...?']\n > Processing time: 0.5989968776702881\n > Real-time factor: 0.3508255724774185\n > Text splitted to sentences.\n['All of a sudden']\n > Processing time: 0.4944484233856201\n > Real-time factor: 0.3274443697637231\n > Text splitted to sentences.\n['I grew horns']\n > Processing time: 0.5917227268218994\n > Real-time factor: 0.3265790480181939\n > Text splitted to sentences.\n['Am I really human?']\n > Processing time: 0.5918266773223877\n > Real-time factor: 0.3184736976512751\n > Text splitted to sentences.\n['Mom, about this...']\n > Processing time: 0.701024055480957\n > Real-time factor: 0.32278609303698425\n > Text splitted to sentences.\n['Oh, Mor- Ning Honey.']\n > Processing time: 1.2274980545043945\n > Real-time factor: 0.3432026285988778\n > Text splitted to sentences.\n['This thing on my head ...']\n > Processing time: 1.3988063335418701\n > Real-time factor: 0.3511987572257952\n > Text splitted to sentences.\n[\"It's horns.\"]\n > Processing time: 0.520362138748169\n > Real-time factor: 0.3132914252784274\n > Text splitted to sentences.\n['You know this?']\n > Processing time: 0.4074397087097168\n > Real-time factor: 0.2997879597253489\n > Text splitted to sentences.\n['But why did they...']\n > Processing time: 0.5124859809875488\n > Real-time factor: 0.3174246033925689\n > Text splitted to sentences.\n[\"Ah, that's because you're not a human.\"]\n > Processing time: 1.2880265712738037\n > Real-time factor: 0.35437444969788595\n > Text splitted to sentences.\n['Since your Dad']\n > Processing time: 0.5336499214172363\n > Real-time factor: 0.3125526128147594\n > Text splitted to sentences.\n['Is a Dragon']\n > Processing time: 0.5604534149169922\n > Real-time factor: 0.317457814398882\n > Text splitted to sentences.\n[\"According to your Dad, your body will start to change when you're 15.\"]\n > Processing time: 1.4979972839355469\n > Real-time factor: 0.35343734068201943\n > Text splitted to sentences.\n[\"I didn't expect the horns to grow overnight though.\"]\n > Processing time: 1.4919993877410889\n > Real-time factor: 0.35202219760840403\n > Text splitted to sentences.\n['Anyways, eat your breakfast.']\n > Processing time: 0.9549984931945801\n > Real-time factor: 0.33841792194234527\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Download contents","metadata":{}},{"cell_type":"markdown","source":"## Download separate files","metadata":{}},{"cell_type":"markdown","source":"### Download json","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"json_results\", f\"{json_output_dir}/*.json\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download image results","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"image_results\", result_image_output_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download Audio","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"audio_results\", \"/kaggle/working/audio_output\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:54:43.786794Z","iopub.execute_input":"2024-11-01T21:54:43.787537Z","iopub.status.idle":"2024-11-01T21:54:43.866430Z","shell.execute_reply.started":"2024-11-01T21:54:43.787483Z","shell.execute_reply":"2024-11-01T21:54:43.865561Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/audio_results.zip","text/html":"<a href='audio_results.zip' target='_blank'>audio_results.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/audio_results.zip","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:54:40.514278Z","iopub.execute_input":"2024-11-01T21:54:40.515217Z","iopub.status.idle":"2024-11-01T21:54:41.579264Z","shell.execute_reply.started":"2024-11-01T21:54:40.515173Z","shell.execute_reply":"2024-11-01T21:54:41.577888Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### Download transcript","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\ndownload_file(\"transcript\", \"/kaggle/working/result/transcript.txt\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download all","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -r {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n    \n\n!cp -r result_dir /\ndownload_file(\"result\", result_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Edit kaggle","metadata":{}},{"cell_type":"code","source":"process_file = '''\nimport json\nimport math\nimport os\nfrom PIL import Image, ImageDraw\nfrom utils.utils import sort_files\n\ndef get_digit_number_for_name_format(directory_path, buffer_number: int = 2):\n    image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.heif', '.ico')\n    num_images = len([name for name in os.listdir(directory_path) if name.lower().endswith(image_extensions)])\n     \n    if num_images > 0:\n        number_of_digit_for_name = math.ceil(math.log10(num_images+1)) + buffer_number\n    else: \n        number_of_digit_for_name = buffer_number\n    return number_of_digit_for_name\n\ndef generate_name_format(number_of_digit_for_name: int):\n    return f\"page_{{:0{number_of_digit_for_name}}}_panel_{{:0{number_of_digit_for_name}}}_bubble_{{:0{number_of_digit_for_name}}}{{}}\"\n\ndef read_coordinates(json_file_path: str):\n    \"\"\"Read essential text coordinates from the given JSON file.\"\"\"\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n    return [\n        text for text, is_essential in zip(data[\"texts\"], data[\"is_essential_text\"]) if is_essential\n    ], data[\"panels\"]  # Return text coordinates and panel coordinates\n\ndef process_full_page(images_dir: str, json_file_path: str, save_path: str = \"./panel_images\", name_format: str = \"page_{:03}_panel_{:03}_bubble_{:03}{}\"):\n    os.makedirs(save_path, exist_ok=True)\n\n    # Extract the text coordinates\n    text_coords, _ = read_coordinates(json_file_path)\n    \n    total_length_text = len(text_coords)\n    image_name_ext = os.path.basename(images_dir)\n\n    # Split the image name and its extension\n    image_name, image_extension = os.path.splitext(image_name_ext)\n    image_name = int(image_name)\n\n    with Image.open(images_dir) as img:\n        # Create a draw object on the original image copy\n        draw = ImageDraw.Draw(img)\n\n        # Initialize the modified panel image path\n        modified_panel_image_path = os.path.join(save_path, name_format)\n        text_order = total_length_text\n\n        # Save the modified panel image with the last computed bubble index\n        img.save(modified_panel_image_path.format(image_name, 0, text_order, image_extension))\n\n        # Loop through each set of coordinates in reverse order\n        for box_index, box in enumerate(reversed(text_coords)):\n            x1, y1, x2, y2 = map(int, box)  # Ensures all values are integers\n\n            # Initialize max color and max value\n            max_color = (0, 0, 0)  # Start with black\n            max_value = 0\n\n            # Iterate through the box's pixels\n            for y in range(y1, y2):\n                for x in range(x1, x2):\n                    # Get the pixel color using absolute coordinates\n                    color = img.getpixel((x, y))\n                    color_value = sum(color[:3])  # Sum RGB values\n\n                    # Update max color if the current pixel's value is higher\n                    if color_value > max_value:\n                        max_value = color_value\n                        max_color = color\n\n            # Fill the rectangle with the max color\n            draw.rectangle(box, fill=max_color)\n\n            # Save modified image path for the current chat bubble\n            text_order = total_length_text - (box_index + 1)\n            modified_image_path = modified_panel_image_path.format(image_name, 0, text_order, image_extension)\n\n            # Instead of saving here, save it once at the end\n            img.save(modified_image_path, format='JPEG')\n\n    print(f\"Processed and saved modified full page images for: {image_name_ext} using json file: {json_file_path}\")\n\ndef process_panel_view(images_dir: str, json_file_path: str, save_path: str = \"./panel_images\", name_format: str = \"page_{:03}_panel_{:03}_bubble_{:03}{}\"):\n    os.makedirs(save_path, exist_ok=True)\n\n    text_coords, panel_coords = read_coordinates(json_file_path)\n    total_length_text = len(text_coords)\n    image_name_ext = os.path.basename(images_dir)\n\n    # Split the image name and its extension\n    image_name, image_extension = os.path.splitext(image_name_ext)\n    image_name = int(image_name)\n\n    with Image.open(images_dir) as img:\n        # Create a copy of the image for drawing\n        original_img = img.copy()\n\n        # Loop through each panel\n        for panel_index, panel in enumerate(panel_coords):\n            # Create a panel-specific image by cropping the original image\n            panel_box = (panel[0], panel[1], panel[2], panel[3])  # box format: (x1, y1, x2, y2)\n            panel_image = original_img.crop(panel_box)\n\n            # Create a draw object on the panel image copy\n            draw = ImageDraw.Draw(panel_image)\n\n            # Initialize the modified panel image path\n            modified_panel_image_path = os.path.join(save_path, name_format)\n            text_order = total_length_text\n\n            # Save the modified panel image with the last computed bubble index\n            panel_image.save(modified_panel_image_path.format(image_name, panel_index, text_order, '.jpg'))\n            \n            # Loop through each set of coordinates in reverse order\n            for box_index, box in enumerate(reversed(text_coords)):\n                # Calculate the center point of the chatbox\n                x_center = (box[0] + box[2]) // 2\n                y_center = (box[1] + box[3]) // 2\n\n                # Check if the center point is inside the panel\n                if panel[0] <= x_center <= panel[2] and panel[1] <= y_center <= panel[3]:\n                    # Calculate the position of the box relative to the panel\n                    relative_box = (\n                        box[0] - panel[0],\n                        box[1] - panel[1],\n                        box[2] - panel[0],\n                        box[3] - panel[1]\n                    )\n\n                    # Get the coordinates of the relative box and convert to integers\n                    x1, y1, x2, y2 = map(int, relative_box)  # Ensures all values are integers\n\n                    # Initialize max color and max value\n                    max_color = (0, 0, 0)  # Start with black\n                    max_value = 0\n\n                    # Iterate through the box's pixels\n                    for y in range(y1, y2):\n                        for x in range(x1, x2):\n                            # Get the pixel color using absolute coordinates\n                            color = original_img.getpixel((x + panel[0], y + panel[1]))\n                            color_value = sum(color[:3])  # Sum RGB values\n\n                            # Update max color if the current pixel's value is higher\n                            if color_value > max_value:\n                                max_value = color_value\n                                max_color = color\n\n                    # Fill the rectangle with the max color\n                    draw.rectangle(relative_box, fill=max_color)\n\n                    # Save modified image path for the current chat bubble\n                    text_order = total_length_text - (box_index + 1)\n                    modified_image_path = modified_panel_image_path.format(image_name, panel_index, text_order, '.jpg')\n\n                    # Instead of saving here, save it once at the end\n                    panel_image.save(modified_image_path, format='JPEG')\n\n    print(f\"Processed and saved modified panel view images for: {image_name_ext}\")\n\ndef process_all_images_and_jsons(images_folder: str, json_folder: str, save_path: str = \"./panel_images\", name_format: str = \"page_{:03}_panel_{:03}_bubble_{:03}{}\", nuke: bool = False, panel_view: bool = False):\n    \n    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'}\n    if nuke:\n        try:\n            for filename in os.listdir(save_path):\n                # Check if the file has an image extension\n                if os.path.splitext(filename)[1].lower() in image_extensions:\n                    file_path = os.path.join(save_path, filename)\n                    os.remove(file_path)\n            print(f\"Deleted images\")\n        except FileNotFoundError as e:\n            print(f\"Error: {e}\")\n\n    # Sort files using the existing sort_files function\n    sorted_image_files = sort_files(images_folder)\n    sorted_json_files = sort_files(json_folder)\n    \n    # Get all image and json file names\n    image_files = [f for f in sorted_image_files if os.path.splitext(f)[1].lower() in image_extensions]\n    json_files = [f for f in sorted_json_files if f.lower().endswith('.json')]\n\n    if len(image_files) != len(json_files):\n        print(f\"Number of images and json files do not match! Length of images: {len(image_files)}, Length of jsons: {len(json_files)}\")\n    else:\n        for image_file in image_files:\n            # Construct full image path\n            image_path = os.path.join(images_folder, image_file)\n\n            # Corresponding json file name (assuming they have the same base name)\n            json_file_name = os.path.splitext(image_file)[0] + '.json'\n            if json_file_name in json_files:\n                json_path = os.path.join(json_folder, json_file_name)\n\n                # Process the image and json based on panel_view flag\n                if panel_view:\n                    process_panel_view(image_path, json_path, save_path, name_format)\n                else:\n                    process_full_page(image_path, json_path, save_path, name_format)\n            else:\n                print(f\"JSON file not found for image: {image_file}\")\n\n\nif __name__ == \"__main__\":\n    images_folder = 'magi_functional/data_test/personal_data/Ruri_Dragon/raw'\n    json_folder = 'magi_functional/data_test/personal_data/Ruri_Dragon/json_results'\n    save_path = \"magi_functional/data_test/code/test_lab/panel_images_full_chapter\"\n    number_of_digit_for_name = get_digit_number_for_name_format(directory_path = images_folder, buffer_number = 2)\n    name_format =generate_name_format(number_of_digit_for_name)\n\n    nuke_option = True\n    panel_view_option= False\n\n    process_all_images_and_jsons(images_folder = images_folder, json_folder = json_folder, save_path = save_path, name_format = name_format, nuke = nuke_option, panel_view = panel_view_option)\n\n'''\n\nwith open(\"/kaggle/working/manga_read_along/magi_functional/utils/process_raw_from_json.py\", \"w\") as file:\n    file.write(process_file)\n    print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:40:13.441141Z","iopub.execute_input":"2024-10-31T16:40:13.441508Z","iopub.status.idle":"2024-10-31T16:40:13.457087Z","shell.execute_reply.started":"2024-10-31T16:40:13.441474Z","shell.execute_reply":"2024-10-31T16:40:13.456154Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}]},{"cell_type":"code","source":"main_file = '''\nimport os\nfrom utils.img2mp4 import create_video_from_images\nfrom utils.process_raw_from_json import process_all_images_and_jsons, get_digit_number_for_name_format, generate_name_format\nfrom utils.utils import rename_image_to_correct_format\n\ndef main(process_image=True, create_video=True):\n    images_folder = 'test_output_coloring'\n    json_folder = 'magi_functional/data_test/personal_data/Ruri_Dragon/json_results'\n\n    save_path = \"test_output_final\"\n    number_of_digit_for_name = get_digit_number_for_name_format(images_folder)\n    name_format = generate_name_format(number_of_digit_for_name)\n\n    formated_image_path = os.path.join(images_folder, 'renamed')\n    formated_images_folder = rename_image_to_correct_format(images_folder, formated_image_path, num_digits=number_of_digit_for_name)\n    formated_json_path = os.path.join(json_folder, 'renamed')\n    formated_json_folder = rename_image_to_correct_format(json_folder, formated_json_path, num_digits=number_of_digit_for_name)\n\n    nuke_option = True\n    panel_view_option = False\n\n    if process_image:\n        process_all_images_and_jsons(images_folder = formated_images_folder, json_folder = formated_json_folder, save_path = save_path, name_format = name_format, nuke = nuke_option, panel_view = panel_view_option)\n\n    if create_video:\n        create_video_from_images(image_dir=save_path, output_video_path=save_path, name_format=name_format, use_padding=True)\n\nif __name__ == \"__main__\":\n    main(process_image=True, create_video=True)\n'''\n\nwith open(\"/kaggle/working/manga_read_along/magi_functional/main.py\", \"w\") as file:\n    file.write(main_file)\n    print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:40:16.519758Z","iopub.execute_input":"2024-10-31T16:40:16.520536Z","iopub.status.idle":"2024-10-31T16:40:16.527484Z","shell.execute_reply.started":"2024-10-31T16:40:16.520499Z","shell.execute_reply":"2024-10-31T16:40:16.526566Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}]},{"cell_type":"code","source":"utils = '''\nimport math\nimport os\nimport re\nimport shutil\n\n\ndef get_digit_number_for_name_format(directory_path, buffer_number: int = 2):\n    image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.heif', '.ico')\n    num_images = len([name for name in os.listdir(directory_path) if name.lower().endswith(image_extensions)])\n     \n    if num_images > 0:\n        number_of_digit_for_name = math.ceil(math.log10(num_images+1)) + buffer_number\n    else: \n        number_of_digit_for_name = buffer_number\n    return number_of_digit_for_name\n\ndef fk_you(number_of_digit_for_name: int):\n    name_format = f\"page_{{:0{number_of_digit_for_name}}}_panel_{{:0{number_of_digit_for_name}}}_bubble_{{:0{number_of_digit_for_name}}}{{}}\"\n    return name_format\n\ndef rename_image_to_correct_format(images_folder, new_folder, num_digits: int =3):\n    if os.path.exists(new_folder):\n        shutil.rmtree(new_folder)\n\n    os.makedirs(new_folder, exist_ok=True)\n\n    sorted_images = sort_files(images_folder)\n\n    for idx, image_path in enumerate(sorted_images):\n        original_path = os.path.join(images_folder, image_path)\n        if os.path.isfile(original_path):  # Ensure it's a file\n            original_ext = os.path.splitext(image_path)[1]\n            new_name = f\"{idx:0{num_digits}}{original_ext}\"\n            new_path = os.path.join(new_folder, new_name)\n            shutil.copy2(original_path, new_path)\n    return new_folder\n\ndef sort_files(directory):\n    files = []\n    \n    for file in os.listdir(directory):\n        if os.path.isfile(os.path.join(directory, file)):\n            files.append(file)\n\n    sorted_list = sorted(files, key=lambda filename: [int(part) if part.isdigit() else part for part in re.split(r'(\\d+)', filename)])\n    return sorted_list\n'''\n\nwith open(\"/kaggle/working/manga_read_along/utils.py\", \"w\") as file:\n    file.write(utils)\n    print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T21:11:08.222786Z","iopub.execute_input":"2024-11-01T21:11:08.223646Z","iopub.status.idle":"2024-11-01T21:11:08.230723Z","shell.execute_reply.started":"2024-11-01T21:11:08.223607Z","shell.execute_reply":"2024-11-01T21:11:08.229808Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}]}]}