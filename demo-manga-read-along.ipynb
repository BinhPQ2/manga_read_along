{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nuke button","metadata":{}},{"cell_type":"code","source":"# Delete all files and folders in the working directory. Use with caution!\nimport shutil\nshutil.rmtree(\"/kaggle/working\", ignore_errors=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clone repo","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/BinhPQ2/manga_read_along.git\n# !git clone -b magiv2 https://github.com/BinhPQ2/manga_read_along.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/manga_read_along\n\nraw_image_path = \"input/raw\"\ncharacter_path = \"input/character\"\n\nraw_image_rename_path = \"output/renamed\"\ncolorized_path = \"output/colorized\"\njson_path = \"output/json\"\ntranscript_path = \"output/transcript\"\naudio_path = \"output/audio\"\nfinal_output_path =\"output/output_final\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git submodule init\n!git submodule update --remote","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat /kaggle/working/manga_read_along/magi_functional/requirements_kaggle.txt | xargs -n 1 pip install -q","metadata":{"execution":{"iopub.status.idle":"2024-11-06T18:02:13.608296Z","shell.execute_reply.started":"2024-11-06T17:59:15.218551Z","shell.execute_reply":"2024-11-06T18:02:13.607330Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\nalbucore 0.0.17 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\nalbumentations 1.4.17 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\narviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nbayesian-optimization 1.5.1 requires numpy>=1.25, but you have numpy 1.22.0 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.22.0 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ncudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\ncudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-cuda 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\ndask-cudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\ndask-cudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-expr 1.1.15 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\ndipy 1.9.0 requires numpy>=1.22.4, but you have numpy 1.22.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmizani 0.11.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nmizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nmne 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.0 which is incompatible.\nnumexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.22.0 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npylibraft 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\npywavelets 1.6.0 requires numpy<3,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\nraft-dask 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\nrmm 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nscikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nstatsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.22.0 which is incompatible.\ntensorflow 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.0 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nucx-py 0.39.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nucxx 0.39.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nxarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\nxarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\nxarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmne 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nscikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ncudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-cudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nplotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nxarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\ntts 0.22.0 requires pandas<2.0,>=1.4, but you have pandas 2.2.2 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nfrom transformers import AutoModel\nimport torch\nimport os\nimport random\nimport re\nimport shutil\nimport argparse\nimport torch\nfrom TTS.api import TTS\nfrom unittest.mock import patch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check GPU and set device","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\nif torch.cuda.is_available():\n    !nvidia-smi\n    !nvcc --version\nelse:\n    print(\"GPU is not available\")\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:09:29.500348Z","iopub.execute_input":"2024-11-06T18:09:29.501270Z","iopub.status.idle":"2024-11-06T18:09:34.804845Z","shell.execute_reply.started":"2024-11-06T18:09:29.501230Z","shell.execute_reply":"2024-11-06T18:09:34.803464Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Wed Nov  6 18:09:33 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             25W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Wed_Nov_22_10:17:15_PST_2023\nCuda compilation tools, release 12.3, V12.3.107\nBuild cuda_12.3.r12.3/compiler.33567101_0\nRunning on cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize model","metadata":{}},{"cell_type":"markdown","source":"## Download MAGI model","metadata":{}},{"cell_type":"code","source":"magiv2 = AutoModel.from_pretrained(\"ragavsachdeva/magiv2\", trust_remote_code=True).to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:10:09.631988Z","iopub.execute_input":"2024-11-06T18:10:09.633204Z","iopub.status.idle":"2024-11-06T18:11:27.076099Z","shell.execute_reply.started":"2024-11-06T18:10:09.633159Z","shell.execute_reply":"2024-11-06T18:11:27.075081Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/13.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60e70250beb448eaa328e5e5f866f610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_magiv2.py:   0%|          | 0.00/1.70k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba100957c0241cc8315145ab9cd4ed8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- configuration_magiv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modelling_magiv2.py:   0%|          | 0.00/34.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e12a726a9245ec9188c1e0241f77b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"utils.py:   0%|          | 0.00/16.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723b8cac254f4bbbbad877492a542767"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- utils.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"processing_magiv2.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203b56455912450cb796ba04ee095c59"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- processing_magiv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- modelling_magiv2.py\n- utils.py\n- processing_magiv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac099a93242426188ae3314555296c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee51423d5f534419a0d0bf8f8be32a3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4882bcf1af944ba9ae6fe120e098e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c36d096167b412bacdea6a44536f6e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cebe5a36ac0f48c7881620dac161f279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4d3ba4c6454e928b22f78ef577421d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nVisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39eb0178f8164ead98f4e7dd180998cb"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Download weight for colorization","metadata":{}},{"cell_type":"code","source":"!gdown 1qmxUEKADkEM4iYLp1fpPLLKnfZ6tcF-t -O /kaggle/working/manga_read_along/manga-colorization-v2-custom/networks/generator.zip\n!mkdir /kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models\n!gdown 161oyQcYpdkVdw8gKz_MA8RD-Wtg9XDp3 -O /kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models/net_rgb.pth","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:11:43.169307Z","iopub.execute_input":"2024-11-06T18:11:43.169757Z","iopub.status.idle":"2024-11-06T18:11:59.679324Z","shell.execute_reply.started":"2024-11-06T18:11:43.169709Z","shell.execute_reply":"2024-11-06T18:11:59.678346Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1qmxUEKADkEM4iYLp1fpPLLKnfZ6tcF-t\nFrom (redirected): https://drive.google.com/uc?id=1qmxUEKADkEM4iYLp1fpPLLKnfZ6tcF-t&confirm=t&uuid=b8e27ce6-40aa-4a91-a04e-4dae4bb50a03\nTo: /kaggle/working/manga_read_along/manga-colorization-v2-custom/networks/generator.zip\n100%|████████████████████████████████████████| 129M/129M [00:03<00:00, 35.4MB/s]\nmkdir: cannot create directory '/kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models': File exists\nDownloading...\nFrom: https://drive.google.com/uc?id=161oyQcYpdkVdw8gKz_MA8RD-Wtg9XDp3\nTo: /kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models/net_rgb.pth\n100%|███████████████████████████████████████| 3.44M/3.44M [00:00<00:00, 211MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Download weight for Text-to-Speech model","metadata":{}},{"cell_type":"code","source":"\n\n# Mock input to automatically respond with 'y'\nwith patch('builtins.input', return_value='y'):\n    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:14:07.205975Z","iopub.execute_input":"2024-11-06T18:14:07.206419Z","iopub.status.idle":"2024-11-06T18:15:34.224206Z","shell.execute_reply.started":"2024-11-06T18:14:07.206370Z","shell.execute_reply":"2024-11-06T18:15:34.223402Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":" > You must confirm the following:\n | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 1.87G/1.87G [00:44<00:00, 42.8MiB/s]\n100%|██████████| 1.87G/1.87G [00:44<00:00, 41.7MiB/s]\n100%|██████████| 4.37k/4.37k [00:00<00:00, 9.86kiB/s]\n\n100%|██████████| 361k/361k [00:00<00:00, 459kiB/s]\n100%|██████████| 32.0/32.0 [00:00<00:00, 49.9iB/s]\n 45%|████▍     | 3.49M/7.75M [00:00<00:00, 34.9MiB/s]","output_type":"stream"},{"name":"stdout","text":" > Model's license - CPML\n > Check https://coqui.ai/cpml.txt for more info.\n > Using model: xtts\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.speakers = torch.load(speaker_file_path)\n100%|██████████| 7.75M/7.75M [00:13<00:00, 34.9MiB/s]/opt/conda/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\nGPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Magiv2","metadata":{}},{"cell_type":"code","source":"# Test module magiv2 (worked)\n!python /kaggle/working/manga_read_along/magi_functional/magiv2.py --image {raw_image_path} --rename_image {raw_image_rename_path} --character {character_path} --json {json_path} --transcript {transcript_path}","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:12:21.452413Z","iopub.execute_input":"2024-11-06T18:12:21.453339Z","iopub.status.idle":"2024-11-06T18:12:49.103028Z","shell.execute_reply.started":"2024-11-06T18:12:21.453284Z","shell.execute_reply":"2024-11-06T18:12:49.101733Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Running on cuda\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nVisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.71s/it]\nWelcome to the CBC MILP Solver \nVersion: 2.10.3 \nBuild Date: Dec 15 2019 \n\ncommand line - /opt/conda/lib/python3.10/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/7a6c3a1eb7114afc88f91e0d135d1d7a-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /tmp/7a6c3a1eb7114afc88f91e0d135d1d7a-pulp.sol (default strategy 1)\nAt line 2 NAME          MODEL\nAt line 3 ROWS\nAt line 496 COLUMNS\nAt line 2087 RHS\nAt line 2579 BOUNDS\nAt line 2740 ENDATA\nProblem MODEL has 491 rows, 160 columns and 1110 elements\nCoin0008I MODEL read with 0 errors\nOption for timeMode changed from cpu to elapsed\nContinuous objective value is 8.59924 - 0.00 seconds\nCgl0003I 0 fixed, 0 tightened bounds, 27 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 27 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 18 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 9 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 9 strengthened rows, 0 substitutions\nCgl0005I 4 SOS with 40 members\nCgl0004I processed model has 13 rows, 40 columns (40 integer (40 of which binary)) and 76 elements\nCbc0038I Initial state - 0 integers unsatisfied sum - 0\nCbc0038I Solution found of 8.59924\nCbc0038I Before mini branch and bound, 40 integers at bound fixed and 0 continuous\nCbc0038I Mini branch and bound did not improve solution (0.01 seconds)\nCbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 8.59924 - took 0.00 seconds\nCbc0012I Integer solution of 8.5992398 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\nCbc0001I Search completed - best objective 8.599239811676402, took 0 iterations and 0 nodes (0.01 seconds)\nCbc0035I Maximum depth 0, 0 variables fixed on reduced cost\nCuts at root node changed objective from 8.59924 to 8.59924\nProbing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nGomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nKnapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nClique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nMixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nFlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nTwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n\nResult - Optimal solution found\n\nObjective value:                8.59923981\nEnumerated nodes:               0\nTotal iterations:               0\nTime (CPU seconds):             0.01\nTime (Wallclock seconds):       0.01\n\nOption for printingOptions changed from normal to all\nTotal time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n\n100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.97s/it]\n\n\nDone you WEEEEB!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Colorization","metadata":{}},{"cell_type":"code","source":"!python \"/kaggle/working/manga_read_along/manga-colorization-v2-custom/inference_v2.py\" -p {raw_image_rename_path} -des_path \"/kaggle/working/manga_read_along/manga-colorization-v2-custom/denoising/models/net_rgb.pth\" -gen \"/kaggle/working/manga_read_along/manga-colorization-v2-custom/networks/generator.zip\" -s {colorized_path} -ds 0 --gpu","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:15:36.585656Z","iopub.execute_input":"2024-11-06T18:15:36.586483Z","iopub.status.idle":"2024-11-06T18:15:44.091903Z","shell.execute_reply.started":"2024-11-06T18:15:36.586431Z","shell.execute_reply":"2024-11-06T18:15:44.090908Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Processing: output/renamed/001.jpg\nSaved colorized image to: output/colorized/001.jpg\nProcessing: output/renamed/002.jpg\nSaved colorized image to: output/colorized/002.jpg\nProcessing: output/renamed/000.jpg\nSaved colorized image to: output/colorized/000.jpg\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text to Speech","metadata":{}},{"cell_type":"markdown","source":"## Download Text-To-Speech model (library of TTS model is conflicted with Magi model, run this after run Magi for now)","metadata":{}},{"cell_type":"code","source":"voice_bank = \"/kaggle/input/ravdess-emotional-speech-audio/\"\nmale_character = ['teacher']\ntranscript_file = f\"{transcript_path}/transcript.txt\"","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:15:56.325206Z","iopub.execute_input":"2024-11-06T18:15:56.325886Z","iopub.status.idle":"2024-11-06T18:15:56.330308Z","shell.execute_reply.started":"2024-11-06T18:15:56.325846Z","shell.execute_reply":"2024-11-06T18:15:56.329146Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/manga_read_along/magi_functional/text_to_speech.py -i {raw_image_rename_path} -v {voice_bank} -t {transcript_file} -o {audio_path}  -m male_character ","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:15:59.348217Z","iopub.execute_input":"2024-11-06T18:15:59.348619Z","iopub.status.idle":"2024-11-06T18:16:54.917368Z","shell.execute_reply.started":"2024-11-06T18:15:59.348573Z","shell.execute_reply":"2024-11-06T18:16:54.916310Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Running on cuda\n > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n > Using model: xtts\n/opt/conda/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.speakers = torch.load(speaker_file_path)\n/opt/conda/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\nGPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n > Text splitted to sentences.\n['Eh...?']\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n > Processing time: 2.1704838275909424\n > Real-time factor: 0.8126047337404965\n > Text splitted to sentences.\n['All of a sudden']\n > Processing time: 1.054344654083252\n > Real-time factor: 0.33751886792299224\n > Text splitted to sentences.\n['I grew horns']\n > Processing time: 0.8201084136962891\n > Real-time factor: 0.36213133855341184\n > Text splitted to sentences.\n['Am I really human?']\n > Processing time: 0.9328665733337402\n > Real-time factor: 0.3361063389217152\n > Text splitted to sentences.\n['Mom, about this...']\n > Processing time: 1.5902233123779297\n > Real-time factor: 0.36229566909750943\n > Text splitted to sentences.\n['Oh, Mor- Ning Honey.']\n > Processing time: 2.2786009311676025\n > Real-time factor: 0.3498047129626103\n > Text splitted to sentences.\n['This thing on my head ...']\n > Processing time: 1.121250867843628\n > Real-time factor: 0.33643018773067707\n > Text splitted to sentences.\n[\"It's horns.\"]\n > Processing time: 0.5612871646881104\n > Real-time factor: 0.31793007555931035\n > Text splitted to sentences.\n['You know this?']\n > Processing time: 0.4873332977294922\n > Real-time factor: 0.31310312397830137\n > Text splitted to sentences.\n['But why did they...']\n > Processing time: 1.174865484237671\n > Real-time factor: 0.3429594355994578\n > Text splitted to sentences.\n[\"Ah, that's because you're not a human.\"]\n > Processing time: 1.3432955741882324\n > Real-time factor: 0.3372616529747054\n > Text splitted to sentences.\n['Since your Dad']\n > Processing time: 0.628460168838501\n > Real-time factor: 0.3381869075285276\n > Text splitted to sentences.\n['Is a Dragon']\n > Processing time: 0.4683353900909424\n > Real-time factor: 0.3101512299226718\n > Text splitted to sentences.\n[\"According to your Dad, your body will start to change when you're 15.\"]\n > Processing time: 1.58674955368042\n > Real-time factor: 0.34507483488493434\n > Text splitted to sentences.\n[\"I didn't expect the horns to grow overnight though.\"]\n > Processing time: 1.1387364864349365\n > Real-time factor: 0.3369811510346032\n > Text splitted to sentences.\n['Anyways, eat your breakfast.']\n > Processing time: 1.0425641536712646\n > Real-time factor: 0.3337476711447646\nAudio files have been saved to output/audio\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Combine to video","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/manga_read_along/magi_functional/main.py -i {colorized_path} -j {json_path} -a {audio_path} -s {final_output_path} ","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:17:03.337102Z","iopub.execute_input":"2024-11-06T18:17:03.337432Z","iopub.status.idle":"2024-11-06T18:17:11.525432Z","shell.execute_reply.started":"2024-11-06T18:17:03.337398Z","shell.execute_reply":"2024-11-06T18:17:11.524190Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Deleted images\nProcessed and saved modified panel view images for: 000.jpg\nProcessed and saved modified panel view images for: 001.jpg\nProcessed and saved modified panel view images for: 002.jpg\nVideo saved as: output/output_final/video_Padding_True.mp4\nAudio file not found for image: page_000_panel_000_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_001_panel_000_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_001_panel_001_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_001_panel_002_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_001_panel_003_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_002_panel_000_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_002_panel_001_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_002_panel_002_bubble_000.jpg. Appending silence for 1 seconds.\nAudio file not found for image: page_002_panel_003_bubble_000.jpg. Appending silence for 1 seconds.\nffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from 'output/output_final/video_Padding_True.mp4':\n  Metadata:\n    major_brand     : isom\n    minor_version   : 512\n    compatible_brands: isomiso2mp41\n    encoder         : Lavf59.27.100\n  Duration: 00:00:54.75, start: 0.000000, bitrate: 992 kb/s\n  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 860x1250 [SAR 1:1 DAR 86:125], 991 kb/s, 24 fps, 24 tbr, 12288 tbn, 24 tbc (default)\n    Metadata:\n      handler_name    : VideoHandler\n      vendor_id       : [0][0][0][0]\n\u001b[0;33mGuessed Channel Layout for Input Stream #1.0 : mono\n\u001b[0mInput #1, wav, from 'output/output_final/temp_audio.wav':\n  Duration: 00:00:55.19, bitrate: 384 kb/s\n  Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, mono, s16, 384 kb/s\nStream mapping:\n  Stream #0:0 -> #0:0 (copy)\n  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\nPress [q] to stop, [?] for help\nOutput #0, mp4, to 'output/output_final/video_Padding_True_audio.mp4':\n  Metadata:\n    major_brand     : isom\n    minor_version   : 512\n    compatible_brands: isomiso2mp41\n    encoder         : Lavf58.76.100\n  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 860x1250 [SAR 1:1 DAR 86:125], q=2-31, 991 kb/s, 24 fps, 24 tbr, 12288 tbn, 12288 tbc (default)\n    Metadata:\n      handler_name    : VideoHandler\n      vendor_id       : [0][0][0][0]\n  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 24000 Hz, mono, fltp, 69 kb/s\n    Metadata:\n      encoder         : Lavc58.134.100 aac\nframe= 1314 fps=0.0 q=-1.0 Lsize=    7030kB time=00:00:54.74 bitrate=1052.1kbits/s speed= 185x    \nvideo:6630kB audio:378kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.324392%\n\u001b[1;36m[aac @ 0x57b7bf850640] \u001b[0mQavg: 21190.207\nAudio merged successfully into video saved as: output/output_final/video_Padding_True_audio.mp4\nTemporary audio file 'output/output_final/temp_audio.wav' deleted. Original video 'output/output_final/video_Padding_True.mp4' deleted.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Download contents","metadata":{}},{"cell_type":"markdown","source":"## Download separate files","metadata":{}},{"cell_type":"markdown","source":"### Download json","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"json_results\", f\"{json_output_dir}/*.json\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download image results","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"image_results\", result_image_output_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download Audio","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"audio_results\", \"/kaggle/working/manga_read_along/test/audio_output\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/audio_results.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download transcript","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\ndownload_file(\"transcript\", \"/kaggle/working/result/transcript.txt\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download all","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -r {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n    \n\n!cp -r result_dir /\ndownload_file(\"result\", result_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Edit kaggle","metadata":{}},{"cell_type":"code","source":"process_file = '''\nTTS==0.22.0\neinops==0.8.0\nPuLP==2.9.0\nPillow==9.5.0\ngdown==5.2.0\npydub==0.25.1\nnumpy==1.26.4\npandas==2.2.2\n'''\n\nwith open(\"/kaggle/working/manga_read_along/magi_functional/requirements_kaggle.txt\", \"w\") as file:\n    file.write(process_file)\n    print(\"Done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experienmental notebook","metadata":{}},{"cell_type":"markdown","source":"## Magiv2","metadata":{}},{"cell_type":"markdown","source":"#### Create raw and character/names list (should apply rename first dumbass, TODO)","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\n\ndef create_chapter_pages_and_character_bank(manga_folder, character_folder):\n    # Create lists for chapter pages and character bank\n    chapter_pages = []\n    character_bank = {\n        \"images\": [],\n        \"names\": []\n    }\n\n#     Iterate through manga images to create chapter_pages\n    for image_file in os.listdir(manga_folder):\n        if image_file.endswith(('.png', '.jpg', '.jpeg')):  # Check for image file extensions\n            # Extract the page number using regex\n            match = re.search(r'p(\\d+)', image_file)\n            if match:\n                page_number = int(match.group(1))  # Convert to integer for sorting\n                chapter_pages.append((page_number, image_file))  # Store as tuple (page_number, image_file)\n            else:\n                page_number = image_file.rsplit(\".\", 1)[0]\n                chapter_pages.append((page_number, image_file))  # Store as tuple (page_number, image_file)\n\n    # Sort chapter pages by page number\n    chapter_pages.sort(key=lambda x: x[0])\n    chapter_pages = [os.path.join(manga_folder, img[1]) for img in chapter_pages]  # Extract just the filenames after sorting\n\n    # Iterate through character images to create character bank\n    for char_image_file in os.listdir(character_folder):\n        if char_image_file.endswith(('.png', '.jpg', '.jpeg')):  # Check for image file extensions\n            # Split the filename to extract character name\n            char_name = char_image_file.split('_')[0]  # Get the part before the underscore\n            character_bank[\"images\"].append(os.path.join(character_folder, char_image_file))\n            character_bank[\"names\"].append(char_name)\n    return chapter_pages, character_bank\n\n# Define your folders\n# One Piece\n# manga_folder = Path(\"/kaggle/working/magi_functional/data_test/personal_data/One_Piece/raw_manga\")\n# character_folder = Path(\"/kaggle/working/magi_functional/data_test/personal_data/One_Piece/character\")\n\n# Ruri Dragon\nmanga_folder = Path(\"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/raw\")\ncharacter_folder = Path(\"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/character\")\n\n# Get chapter pages and character bank\nchapter_pages_original, character_bank_original = create_chapter_pages_and_character_bank(manga_folder, character_folder)\n\nchapter_pages_test = chapter_pages_original\ncharacter_bank_test = character_bank_original\n\n# Print the results (for debugging)\nprint(\"Chapter Pages:\")\nprint(chapter_pages_test)\n\nprint(\"\\nCharacter Bank:\")\nprint(character_bank_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Process (OCR → Transcript)","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\ndef read_image(path_to_image):\n    with open(path_to_image, \"rb\") as file:\n        image = Image.open(file).convert(\"L\").convert(\"RGB\")\n        image = np.array(image)\n    return image\n\n\nchapter_pages = [read_image(x) for x in chapter_pages_test]\ncharacter_bank = character_bank_test.copy()\ncharacter_bank[\"images\"] = [read_image(x) for x in character_bank_test[\"images\"]]\n\nwith torch.no_grad():\n    per_page_results = magiv2.do_chapter_wide_prediction(chapter_pages, character_bank, use_tqdm=True, do_ocr=True)\n\nprint(\"Continue with next cell\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save transcript and json file","metadata":{}},{"cell_type":"code","source":"result_dir = \"/kaggle/working/result\"\njson_output_dir = f\"{result_dir}/json_results\"\nresult_image_output_dir = f\"{result_dir}/image_results\"\ntranscript_output_dir = f\"{result_dir}/transcript.txt\"\nos.makedirs(json_output_dir, exist_ok=True)  # Create the directory if it doesn't exist  \nos.makedirs(result_image_output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\ntranscript = []\nfor i, (image, page_result) in enumerate(zip(chapter_pages, per_page_results)):\n    image_name_ext = os.path.basename(chapter_pages_test[i]) \n    # Split the image name and its extension\n    image_name, image_extension = os.path.splitext(image_name_ext)\n    \n#     model.visualise_single_image_prediction(image, page_result, os.path.join(result_image_output_dir, f\"{image_name}.png\")) # enable this if you want to see the result image included with all the annotation box\n    # Save page_result to JSON\n    json_file_path = os.path.join(json_output_dir, f\"{image_name}.json\")\n    with open(json_file_path, 'w') as json_file:\n        json.dump(page_result, json_file, indent=4)\n\n    speaker_name = {\n        text_idx: page_result[\"character_names\"][char_idx] for text_idx, char_idx in page_result[\"text_character_associations\"]\n    }\n    \n    transcript.append(f\"<page>{image_name}<endpage>\")\n    for j in range(len(page_result[\"ocr\"])):\n        if not page_result[\"is_essential_text\"][j]:\n            continue\n        name = speaker_name.get(j, \"unsure\") \n        transcript.append(f\"<name>{name}<endname>: {page_result['ocr'][j]}\")\nwith open(transcript_output_dir, \"w\") as fh:\n    for line in transcript:\n        fh.write(line + \"\\n\")\n\nprint(\"\\n\\nDone you WEEEEB!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text to Speech","metadata":{}},{"cell_type":"code","source":"voice_mapping = {\n'other': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-01-01-01-01-02.wav\",\n'ruri': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_04/03-01-01-01-01-01-04.wav\",\n'teacher': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\",\n'mom': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_08/03-01-01-01-01-01-08.wav\",\n'unsure': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_10/03-01-01-01-01-01-10.wav\",\n'ukka': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_12/03-01-01-01-01-01-12.wav\"\n}\n ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from magi_functional.utils.utils import rename_image_to_correct_format, get_digit_number_for_name_format, generate_name_format\n\nimages_folder = '/kaggle/working/manga_read_along/input/colorized'\n\nnumber_of_digit_for_name = get_digit_number_for_name_format(images_folder)\nname_format = generate_name_format(number_of_digit_for_name)\nprint(name_format)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to parse the transcript from a text file\ndef parse_transcript(transcript_path, line_end: int = None):\n    pages = []\n    current_page = None\n\n    with open(transcript_path, 'r') as file:\n        content = file.readlines()\n        \n    lines_to_process = content[:line_end] if line_end is not None else content\n    \n    for line in lines_to_process:\n        if line.startswith(\"<page>\"):\n            # Start a new page\n            if current_page is not None:\n                pages.append(current_page)\n            current_page = {\"page\": re.search(r'<page>(\\d+)<endpage>', line).group(1), \"lines\": []}      \n        elif line.startswith(\"<name>\") and current_page is not None:\n            # Extract character name and dialogue\n            match = re.match(r\"<name>([^<]+)<endname>:\\s*(.+)\", line)\n            if match:\n                character = match.group(1).lower()  # Lowercase for consistency\n                dialogue = match.group(2)\n                current_page[\"lines\"].append((character, dialogue))\n\n    # Add the last page if it exists\n    if current_page is not None:\n        pages.append(current_page)\n        \n    return pages\n\n# Function to get all voice files from the specified directory\ndef get_voice_files(directory):\n    all_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".wav\"):\n                all_files.append(os.path.join(root, file))\n    return all_files\n\n# Function to filter voice files into male and female categories\ndef filter_voice_files(files):\n    male_files = []\n    female_files = []\n    \n    for file in files:\n        # Extract the actor number from the filename\n        match = re.search(r\"(\\d{2})\\.wav$\", file)\n        if match:\n            actor_number = int(match.group(1))\n            if actor_number % 2 == 0:  # Even numbers are female\n                female_files.append(file)\n            else:  # Odd numbers are male\n                male_files.append(file)\n                \n    return male_files, female_files\n\n# Function to randomly select voice files for characters\ndef select_voice_files_for_characters(characters):\n    all_files = get_voice_files(voice_bank)\n    male_files, female_files = filter_voice_files(all_files)\n\n    selected_files = {}\n    used_files = set()  # To keep track of used voice files\n\n    for character in characters:\n        if character in male_characters:\n            # Select a random male voice that hasn't been used\n            available_male_files = list(set(male_files) - used_files)\n            if available_male_files:\n                selected_files[character] = random.choice(available_male_files)\n                used_files.add(selected_files[character])\n        else:\n            # Select a random female voice that hasn't been used, if available\n            available_female_files = list(set(female_files) - used_files)\n            if available_female_files:\n                selected_files[character] = random.choice(available_female_files)\n                used_files.add(selected_files[character])\n            else:\n                # Select any remaining file for other characters, ensuring it's not already used\n                available_files = list(set(all_files) - used_files)\n                if available_files:\n                    selected_files[character] = random.choice(available_files)\n                    used_files.add(selected_files[character])\n    \n    return selected_files\n\n# Function to convert text to speech for a character\ndef voice_character(character, text, page_number, bubble_number, selected_files, save_directory):\n    speaker_wav = selected_files.get(character)\n\n    if speaker_wav:\n        audio_output_filename = name_format.format(int(page_number), 0, bubble_number+1, \".wav\")\n        output_filename = os.path.join(save_directory, audio_output_filename)\n        output = tts.tts_to_file(text=text, speaker_wav=speaker_wav, language=\"en\")\n        os.rename(\"output.wav\", output_filename)  # Rename the default output file\n        return output_filename\n    else:\n        raise ValueError(f\"Character '{character}' not found in voice mapping.\")\n\n# Function to process the transcript and create audio files\ndef text2speech(pages, selected_files, save_directory):\n    output_files = []\n    time_file = []\n    \n    for page in pages: \n        page_number = page[\"page\"]\n        for bubble_number, (character, dialogue) in enumerate(page[\"lines\"]):\n            try:\n                output = voice_character(character, dialogue, page_number, bubble_number, selected_files, save_directory)\n                output_files.append(output)\n            except ValueError as e:\n                print(e)\n\n    return output_files","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the directory where the RAVDESS files are located\nvoice_bank = \"/kaggle/input/ravdess-emotional-speech-audio/\"\n# Define male characters for the check\nmale_characters = ['teacher']\ntranscript_file = f\"{transcript_path}/transcript.txt\"\n\nsave_directory = audio_path\n\nif os.path.exists(save_directory):\n    shutil.rmtree(save_directory)\nos.makedirs(save_directory, exist_ok=True)\n\n# Get unique characters from the transcript\npages = parse_transcript(transcript_file, line_end = 20) # delete line_end to get audio of all page\ncharacters = {line[0] for page in pages for line in page[\"lines\"]}  # Get unique characters\n\n# Select voice files for characters\nselected_files = select_voice_files_for_characters(characters)\n\n# # Call the function to process the transcript and generate audio files\noutput_files = text2speech(pages, selected_files, save_directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}